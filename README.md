# Catch Up On Large Language Models
This article on Towards Data Science provides a practical guide to large language models (LLMs) without the hype, authored by Marco Peixeiro. It aims to help readers understand the fundamentals of LLMs, specifically focusing on the Transformer architecture and its role in LLMs. The article covers tokenization, word embeddings, word order, and the inner workings of the encoder and decoder components in LLMs. It also explains how LLMs are trained, distinguishing between encoder-only, decoder-only, and encoder-decoder models. The latter part of the article includes a hands-on project demonstrating sentiment analysis using Flan-T5, an improved version of the T5 model. The project involves setting up the environment, loading and analyzing financial news data, using the Flan-T5 model for sentiment analysis, and evaluating the model's performance through confusion matrices and classification reports. The article concludes by encouraging readers to explore and experiment with LLMs.
